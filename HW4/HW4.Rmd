---
title: "HW4"
author: "Jon Griffith and Lauren Quesada"
date: "2025-04-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 10

```{r}
library(ISLR2)
```

### (a)

```{r}
df <- Carseats

fit <- lm(Sales ~ Price + Urban + US, data=df)
summary(fit)
```

### (b)

The coefficient for the intercept, $\hat{\beta}_0 = 13.043$, can be interpreted as the predicted sales, on average, for a carseat that costs \$0, is not in an Urban environment, and is not in the US, is 13,043 carseats.

The coefficient for 'Price', $\hat{\beta}_1 = -0.054$, can be interpreted as for each dollar increase in Price, you can expect the number of carseats sold to go down by approximately 54 units, with the other variables held constant. 

The coefficient for 'UrbanYes', $\hat{\beta}_2 = -0.022$, can be interpreted as stores located in urban areas sell, on average, approximately 22 fewer units than stores located in rural areas, with the other variables held constant. However, since this term has a high p-value, meaning it is not statistically significant, we should discount this variable's effect on the response.

The coefficient for 'USYes', $\hat{\beta}_3 = 1.2$, can be interpreted as stores located in the US sell, on average, approximately 1200 more units than stores not located in the US, with the other variables held constant.

The 'Price' and 'US' variables are both statistically significant while the 'Urban' variable is not.

### (c)

$Sales = 13.043 - 0.054 (\text{Price}) - 0.022 (I_{Urban='Yes'}) + 1.2 (I_{US='Yes'})$

### (d)

We can reject the null hypothesis for the 'Price' and 'US' variables, based on the significance codes provided in the model summary.

### (e)

```{r}
fit2 <- lm(Sales ~ Price + US, data = df)
summary(fit2)
```

### (f)

RSE: From model (a) to (e), the RSE decreased from 2.472 to 2.469.

R2: From model (a) to (e), the R2 remained the same. The adjusted-R2 however, increased from 0.2335 to
0.2354.

F-statistic: From model (a) to (e), the F-statistic increased from 41.52 to 62.43, and both models returned
statistically significant F-statistics.

Based on these metrics, model (e) fits the data better than model (a), admittedly by a small margin. Neither
suggest a particularly strong fit between the estimated linear model and the data.

Since the first model has an adjusted R-squared of 0.2335 while the second model has an adjusted R-squared of 0.2354, this may be the best metric to suggest a better fit for the second model with one less variable. This also means that each model accounts for about 23.54 percent of variation in the predictions for sales. 

Overall, whether this is a good or bad fit is subjective, but it seems like a poor fit since the goal here is presumably to make business decisions based on predicted unit sales. If these predictions aren't very accurate, you would not want to make important decisions based on these models.

### (g)

```{r}
confint(fit2, level=0.95)
```

The $95\%$ confidence interval for $\beta_0$, the intercept coefficient, is approx. $(11.79, 14.27)$.

The $95\%$ confidence interval for $\beta_1$, the coefficient for the predictor Price, is approx. $(-0.06,-0.04)$.

The $95\%$ confidence interval for $\beta_2$, the coefficient for the predictor US, is approx. $(0.69, 1.71)$.

### (h)

```{r}
par(mfrow=c(2,2))
plot(fit2)
#plot(fit2, which=5)
```

Observations 69, 377, and 51 are potential outliers, though they fall within the ± 3 studentized residuals
rule of thumb. Further investigation should be taken with these three points. It also appears there may be a potential high leverage observation as identified in the Resids vs. Leverage plot, though it does not cross the threshold for Cook's distance so again, this would be a good point to investigate but doesn't immediately stand out as a problem point.


## 13

### (a) - (c)

```{r}
set.seed(1)

x <- rnorm(100)
eps <- rnorm(100, 0, sd=sqrt(0.25))
y <- -1 + 0.5*x + eps
```

Vector 'y' has a length of 100. For this model, $\beta_0 = -1$ and $\beta_1 = 0.5$. 

### (d)

```{r}
plot(x, y)
```
We observe a positive linear relationship between x and y, which accurately reflects what we know the true function to equal.


### (e)

```{r}
fit_e <- lm(y ~ x)
summary(fit_e)
```

The model has a statistically significant $\hat{\beta}_0 = -1.01885$ for the intercept and $\hat{\beta}_1 = 0.49947$ for 'x', which accurately reflects the true coefficients of $\beta_0 = -1$ and $\beta_1 = 0.5$. We also see that the R-squared is approximately 0.4674 which suggests that 'x' accounts for approximately 46.7 percent of the explained variance for the predictions of 'y'.

With the near zero p-value for x and an F-statistic of 38.64, this further suggests that x is a statistically significant variable.



### (f)

```{r}
plot(x,y,
     main='Least Squares Line vs Population Regression Line')
abline(fit_e, col='red', lwd=2)
abline(a=-1, b=0.5, col='blue', lwd=2)
legend('topleft', c('Least Squares Line', 'Population Regression Line'), col=c('red', 'blue'), lwd=2)
```

### (g)

```{r}
fit4 <- lm(y ~ x + I(x^2))
summary(fit4)
```

There is no evidence that the quadratic term improves the fit of the model. We conclude this based on a high p-value that doesn't beat any standard threshold, therefore we fail to reject the null hypothesis $H_0: \beta_2 = 0$. We also only observe a marginal increase in the adjusted R-squared as further evidence that this does not do much to improve the fit. This makes sense since the true equation is not quadratic.


### (h)

```{r}
set.seed(1)

x <- rnorm(100)
eps <- rnorm(100, 0, sd=0.01)
y <- -1 + 0.5*x + eps

fit_h <- lm(y ~ x)

summary(fit_h)
```

```{r}

plot(x,y,
     main='Least Squares Line vs Population Regression Line')
abline(fit_h, col='red', lwd=2)
abline(a=-1, b=0.5, col='blue', lwd=2)
legend('topleft', c('Least Squares Line', 'Population Regression Line'), col=c('red', 'blue'), lwd=2)
```

We see that we have similar values for both $\hat{\beta}_0$ and $\hat{\beta}_1$ which very closely approximate their true values. The variation is now even more explained by this model with an increase in proportion comparable to the decrease in variance for the true function. From the plot, we see that the OLS line is an even better approximation of the population regression line than we saw in the previous model, which makes sense since the data points are tighter around the line due to lower random error.


### (i)

```{r}
set.seed(1)

x <- rnorm(100)
eps <- rnorm(100, 0, 1)
y <- -1 + 0.5*x + eps

fit_i <- lm(y ~ x)

summary(fit_i)
```

```{r}

plot(x,y,
     main='Least Squares Line vs Population Regression Line')
lines(x, fit_i$fitted.values)
lines(x, -1 + 0.5*x, col='blue', lwd=2)
legend('topleft', c('Least Squares Line', 'Population Regression Line'), col=c('red', 'blue'), lwd=2)
```


We once again see similar coefficient estimates for $\hat{\beta}_0$ and $\hat{\beta}_1$ as the previous two models, which closely approximates the true values. We also see a comparable decrease in $R^2$ proportional to variance increase in the error term. As the error term variance goes up, we see that we now only explain approximately 17.96. For the plot, we now see that the lines can be differentiated moreso than all previous models meaning that the OLS fit is slightly less accurate (though still very accurate) than the previous models. This makes sense with the increase in the variance for the error term resulting in a larger spread of data points.

```{r}
cat("Second fit with e ~ N(0, 0.001) \n")
confint(fit_h)
cat('\n')

cat("First fit with e ~ N(0, 0.25) \n")
confint(fit_e)
cat('\n')

cat("Third fit with e ~ N(0, 1) \n")
confint(fit_i)
```

We put the models in ascending order in terms of variance for the error term, with the second fit first, the original fit second, and the third fit third. We see that there is a positive correlation with variance in the error term and width of the confidence interval. That is, as the data set becomes more noisy, the range of values in the confidence interval becomes wider. This makes sense since the proportion of the variance explained by the model is also going down, meaning we become less confident in our ability to explain the variance in the response variable y, which also corresponds to a wider range of values for our confidence interval.


## 14

### (a)

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100) / 10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

The linear model has the form
\begin{align*}
  Y &= \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon \sim N(0,1)\\
  Y &= 2 + 2X_1 + 0.3X_2 + \epsilon
\end{align*}

### (b)

```{r}
plot(x1,x2,
     main = paste('x1 VS x2\nCorr = ', round(cor(x1,x2),3)))

```

### (c)

```{r}
fit_14c <- lm(y ~ x1 + x2)
summary(fit_14c)
```

Overall, we see that an additive linear model with variables X1 and X2 is not a good fit for the responses.
The R2 value is 0.2, which is relatively low. The F-statistic is not very large (it is 12.8), and the RSE is
close to 1 (it is 1.056).

$\hat{\beta}_0 \approx 2.13$, which is an overestimate of $\hat{\beta}_0 = 2.0$.

$\hat{\beta}_1 \approx 1.44$, which is an overestimate of $\hat{\beta}_1 = 2.0$.

$\hat{\beta}_2 \approx 1.01$, which is an overestimate of $\hat{\beta}_1 = 0.3$.

With a p-value of 0.0487, we can reject the null hypothesis $H0 : \hat{\beta}_1 = 0$ at the 0.05 significance level and
suggest $\hat{\beta}_1 \neq 0$. With a p-value of 0.3754, we fail to reject the null hypothesis $H0 : \hat{\beta}_2 = 0$–which agrees with
the earlier simplification of the linear model.


### (d)

```{r}
fit_14d <- lm(y ~ x1)
summary(fit_14d)
```

This fit has an even lower p-value for $X_2$ showing that we can still reject the null hypothesis, but this time with a coefficient that approximately matches the true $\beta_1$. It makes sense that the coefficient estimate would match the true value in this case since $X_1$ is independent of $X_2$. 


### (e)

```{r}
fit_14e <- lm(y ~ x2)
summary(fit_14e)
```

We see that $X_2$ is now statistically significant meaning we can reject the null that $\beta_2 = 0$. However, the coefficient estimate for $X_2$ does not come close to the true value for $\beta_2$. This makes sense since it is highly correlated with and depends on $X_1$. It is therefore trying to signal the magnitude of the $X_1$ variable's impact on $Y$ through $X_2$ since $X_1$ wasn't included in the model.


### (f)

The results in (c)-(e) don't contradict each other since there is a high correlation between $X_1$ and $X_2$. When we include both in the model, we see that $X_1$ impact is being underestimated while $X_2$ is being overestimated, which can be attributed to collinearity. The second model has just $X_1$ which is independent of $X_2$, so we get its isolated impact on the response pretty accurately. The third model with just $X_2$ is statistically significant, but mainly because it is essentially a proxy for both the impact of $X_1$ and itself.

```{r}
fit_14f <- lm(y ~ x1*x2)
summary(fit_14f)
```

### (g)

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)

fitg1 <- lm(y~x1 + x2)
fitg2 <- lm(y~x1)
fitg3 <- lm(y~x2)
```

```{r}
summary(fitg1)
summary(fitg2)
summary(fitg3)
```

```{r}
par(mfrow=c(2,2))
plot(fitg1)
```

```{r}
par(mfrow=c(2,2))
plot(fitg2)
```

```{r}
par(mfrow=c(2,2))
plot(fitg3)
```

We see that the first model now has $X_2$ being statistically significant meaning we can reject the null hypothesis, while $X_1$ is now not statistically significant and we fail to reject the null in its case. Both of the coefficient estimates are still inaccurate.

For the second and third models, we see that both $X_1$ and $X_2$ are both statistically significant for each of their respective regressions, meaning we can reject the null in both cases. For the model with $X_1$, we see that it still comes close to its true $\beta$ estimate, but not as close as it did before this new observation was introduced. The $\beta$ estimate for $X_2$ is still way off for its respective model.

Looking at the residual plots for each model, we see that there is strong evidence across the board that the new observation is a high leverage point in the first model with both variables since it crosses the highest threshold for Cook's distance in the leverage plot. 

The added observation does not show up as a high leverage point in either of the models with just $X_1$ or $X_2$. However, it does appear to have relatively high leverage for the third model with just $X_2$, just not enough to be a concern based on Cook's distance.

For the second model with just $X_1$, we have evidence that the additional point could be an outlier from looking at the QQ plot and the residuals vs fitted plots. However, we don't appear to have any evidence that the added observation is an outlier in the first or third model with both variables and just $X_2$, respectively.


